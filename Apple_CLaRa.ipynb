{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyMyz9zNhceTQGehQwQldHRm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpsbali/GoogleColab/blob/main/Apple_CLaRa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj8_7RqmffPK"
      },
      "outputs": [],
      "source": [
        "%pip install torch\n",
        "%pip install -U transformers==4.57.1 trl==0.25.1 datasets==4.4.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Login into Hugging Face Hub\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token)"
      ],
      "metadata": {
        "id": "l5Vwo-BXfh22"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "import os\n",
        "\n",
        "# Define the model repo ID and the specific subfolder you want\n",
        "REPO_ID = \"apple/CLaRa-7B-Instruct\"\n",
        "SUBFOLDER_NAME = \"compression-16\"\n",
        "\n",
        "# Define the local directory where you want to save the files\n",
        "LOCAL_DIR = \"./local_model_folder\"\n",
        "\n",
        "# Create the local directory if it doesn't exist\n",
        "os.makedirs(LOCAL_DIR, exist_ok=True)\n",
        "\n",
        "# Download only the files within the specified subfolder\n",
        "# The 'allow_patterns' argument takes a list of patterns to match files/folders\n",
        "downloaded_folder_path = snapshot_download(\n",
        "    repo_id=REPO_ID,\n",
        "    local_dir=LOCAL_DIR,\n",
        "    allow_patterns=[f\"{SUBFOLDER_NAME}/*\"],\n",
        "    ignore_patterns=[\"*.safetensors\", \"*.bin\"] # Optional: ignore large files if not needed\n",
        ")\n",
        "\n",
        "print(f\"Downloaded subfolder located at: {downloaded_folder_path}/{SUBFOLDER_NAME}\")"
      ],
      "metadata": {
        "id": "iuzT6zpkfk6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "import os\n",
        "\n",
        "#Construct the full local path to the 'compression-16' model files\n",
        "local_model_path = os.path.join('/content/local_model_folder', 'compression-16')\n",
        "\n",
        "#Load the model and tokenizer from the local path\n",
        "unirag = AutoModel.from_pretrained(\n",
        "    local_model_path,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    local_files_only=True).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained( local_model_path, trust_remote_code=True, local_files_only=True )\n",
        "\n",
        "documents = [ [\n",
        "    \"Weldenia is a monotypic genus of flowering plant in the family Commelinaceae...\",\n",
        "    \"Hagsatera is a genus of flowering plants from the orchid family...\",\n",
        "    \"Alsobia is a genus of flowering plants in the family Gesneriaceae...\" ] ]\n",
        "\n",
        "questions = [ \"Which genus of plant grows originally in Mexico and Guatemala, Phylica or Weldenia?\" ]\n",
        "\n",
        "#Instruction-tuned usage\n",
        "out = unirag.generate_from_text( questions=questions, documents=documents, max_new_tokens=64 )\n",
        "\n",
        "print(\"Generated answer:\", out)"
      ],
      "metadata": {
        "id": "8To1gNZmfrIx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}